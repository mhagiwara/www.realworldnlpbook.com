<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Real-World Natural Language Processing - Sentiment Analysis</title><link href="http://www.realworldnlpbook.com/blog/" rel="alternate"></link><link href="http://www.realworldnlpbook.com/blog/feeds/sentiment-analysis.atom.xml" rel="self"></link><id>http://www.realworldnlpbook.com/blog/</id><updated>2018-10-27T00:00:00-04:00</updated><entry><title>Improving a Sentiment Analyzer using ELMo — Word Embeddings on Steroids</title><link href="http://www.realworldnlpbook.com/blog/improving-sentiment-analyzer-using-elmo.html" rel="alternate"></link><published>2018-10-27T00:00:00-04:00</published><updated>2018-10-27T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:www.realworldnlpbook.com,2018-10-27:/blog/improving-sentiment-analyzer-using-elmo.html</id><summary type="html">&lt;p&gt;In the &lt;a href="training-sentiment-analyzer-using-allennlp.html"&gt;previous post&lt;/a&gt;, I showed how to train a sentiment classifier from the Stanford Sentiment TreeBank. Thanks to a very powerful deep NLP framework, &lt;a href="https://allennlp.org/"&gt;AllenNLP&lt;/a&gt;, we were able to write the entire training pipeline in less than 100 lines of Python code. &lt;/p&gt;
&lt;p&gt;In this post, I'm going to explain …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="training-sentiment-analyzer-using-allennlp.html"&gt;previous post&lt;/a&gt;, I showed how to train a sentiment classifier from the Stanford Sentiment TreeBank. Thanks to a very powerful deep NLP framework, &lt;a href="https://allennlp.org/"&gt;AllenNLP&lt;/a&gt;, we were able to write the entire training pipeline in less than 100 lines of Python code. &lt;/p&gt;
&lt;p&gt;In this post, I'm going to explain how to improve the sentiment analyzer using ELMo.&lt;/p&gt;
&lt;h2&gt;What are Word Embeddings?&lt;/h2&gt;
&lt;p&gt;Before talking about ELMo, let's study word embeddings in depth. What is a word embedding? As I touched upon in the previous post, an embedding in deep learning is a continuous vector representation of something that is usually discrete and high dimensional. In NLP, word embeddings are usually just a mapping table from words to continuous vectors. &lt;/p&gt;
&lt;p&gt;Before the advent of popular word embedding techniques (i.e., word2vec) around 2013, NLP didn't really have nice ways to represent word semantics in a continuous vector space. People used the bag of words (BoW) representation, which is simply a way to map each unique token to a dimension (an axis) in the N-dimensional space by ignoring the word order completely.&lt;/p&gt;
&lt;p&gt;Clearly, BoW has several issues, one of which is its inability to represent semantic similarity (or dissimilarity) between words. As an example, let's consider a hypothetical three dimensional space with just three concepts — "dog", "cat", and "pizza". Because each unique word is mapped to a dimension, the vectors for "dog", "cat", and "pizza" will be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v("dog") = (1, 0, 0)&lt;/li&gt;
&lt;li&gt;v("cat") = (0, 1, 0)&lt;/li&gt;
&lt;li&gt;v("pizza") = (0, 0, 1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;respectively. By the way, those vectors filled with 0s except just one 1 are called one-hot vectors. As you can see, there is no way to know, for example, "dog" and "cat" are related concepts. In the eyes of BoW and one-hot vectors, "dog" is no more similar to "cat" than "pizza" is!&lt;/p&gt;
&lt;p&gt;Word embeddings solve this exact issue by representing words not just by one-hot vectors but by sets of continuous numbers. This is why the use of word embeddings has become so popular in recent years in NLP. The vectors for "dog", "cat", and "pizza" can be, for example,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;v("dog") = (0.8, 0.3, 0.1)&lt;/li&gt;
&lt;li&gt;v("cat") = (0.7, 0.5, 0.1)&lt;/li&gt;
&lt;li&gt;v("pizza") = (0.1, 0.2, 0.8)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first element here represents some concept of "animal-ness" and the third "food-ness". (I'm making these numbers up, but you get the point). Those vectors are learned from a large corpus of natural language text so that words that appear in similar context get assigned similar vectors. By using pre-trained word embeddings instead of one-hot vectors, your model already "knows" how the basic building blocks of the language work. For example, if you wanted to train an "animal name tagger," then all your model has to learn would be to look at just the first element of each word vector and see if the value is high enough. This is a great jump-start from trying to figure out from (1, 0, 0), (0, 1, 0), and (0, 0, 1).       &lt;/p&gt;
&lt;h2&gt;What is ELMo?&lt;/h2&gt;
&lt;p&gt;You may have noticed that word embeddings mentioned above also have another serious issue. A word is assigned the same vector representation no matter where it appears and how it's used, because word embeddings rely on just a look-up table. In other word, they ignore polysemy — a concept that words can have multiple meanings. For example, the word "bank" gets assigned a word vector that is always the same regardless of the context (whether it's a financial institution of a land alongside a river). What if there is a "hot" right before "dog" in a sentence? Suddenly, this "dog" sounds a lot more like "pizza" than "cat"!&lt;/p&gt;
&lt;p&gt;I need to mention that it's not that nothing has been done to address this issue. The original word2vec paper [&lt;a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;Mikolov, et al. 2013&lt;/a&gt;] deals with idiomatic phrases such as "New York" and "Boston Globe" by first detecting those phrases using a data-driven approach and then by treating them as single tokens. There is also a body of work that focuses on learning a vector representation per word sense, not just per word (e.g., [&lt;a href="https://arxiv.org/abs/1504.06654"&gt;Neelakantan et al., 2015&lt;/a&gt;]). In either case, word embeddings are still based on static mapping tables and you'd need a separate phase to disambiguate word senses.&lt;/p&gt;
&lt;p&gt;Human language is something more dynamic. What a word means can depend on what comes before &lt;em&gt;and&lt;/em&gt; after it (and possibly beyond sentence boundaries). This is why you get confused when you see sentences like "&lt;a href="https://en.wikipedia.org/wiki/Garden-path_sentence"&gt;The horse raced past the barn fell.&lt;/a&gt;" It'd be natural to think of a "word embedding on steroids" which reads the whole sentence once and produces word vectors that take into account the entire sentence as context. This is exactly what ELMo does!&lt;/p&gt;
&lt;p&gt;ELMo is a word representation technique proposed by AllenNLP [&lt;a href="https://arxiv.org/abs/1802.05365"&gt;Peters et al. 2018&lt;/a&gt;] relatively recently. Unlike traditional word embedding methods, ELMo is dynamic, meaning that ELMo embeddings change depending on the context even when the word is the same. How can this be possible? In the following sections, I'm going to show how it works.  &lt;/p&gt;
&lt;h2&gt;How ELMo works&lt;/h2&gt;
&lt;p&gt;Instead of relying on mapping tables, ELMo uses a pre-trained language model. That's how the name ELMo got "LM" in it (it stands for Embeddings from Language Models). In general, a language model is a statistical model that gives probabilities to sequences of words, such as phrases and sentences. In deep NLP, recurrent neural networks (RNNs) are often used to train language models. As the RNN reads a sentence word by word, its internal states get updated so that they reflect the "content" of the sentence seen so far.&lt;/p&gt;
&lt;figure style="text-align: center"&gt;
    &lt;img src="images/elmo.png"/&gt;
    &lt;figcaption&gt;Figure: ELMo uses internal representations of multi-layer biLM&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;ELMo uses one particular type of language model called biLM, which is a combination of two LMs in both directions. As you can see in the figure above, there are two "passes" — forward and backward — that scan the sentence in both directions. The internal states from the forward pass at a certain word reflect the word itself &lt;em&gt;and&lt;/em&gt; everything that happened before that word, whereas the ones from the backward pass are computed from the word itself &lt;em&gt;and&lt;/em&gt; everything after that word. The internal states of both passes get concatenated and produce an intermediate word vector. Therefore, this intermediate word vector at that word is still the representation of what the word means, but it "knows" what is happening in the rest of the sentence and how the word is used. &lt;/p&gt;
&lt;p&gt;Another feature of ELMo is that it uses an LM comprised of multiple layers. Those backward and forward passes are stacked together and form a multilayer RNN, as you can see in the figure. The intermediate word vector produced by the layer below is fed into the next layer above. This is repeated as many times as there are layers. This way, you can expect that internal states get processed further as they go up in the layer ladder, and upper layers can represent more abstract semantics (for example, topics and sentiment) compared to what lower layers can capture (for example, part of speech and short phrases). The final representation used by downstream NLP tasks is the weighed combination of those different intermediate word vectors. Specifically, it is the weighted combination of L+1 word vectors, where L is the number of layers. Why +1? Because the input to biLM (raw word embeddings which you can see at the bottom of the figure) get also combined. The weights are learned in a task-dependent way. &lt;/p&gt;
&lt;p&gt;Finally, ELMo uses a character CNN (convolutional neural network) for computing those raw word embeddings that get fed into the first layer of the biLM. The input to the biLM is computed purely from characters (and combinations of characters) within a word, without relying on some form of lookup tables. Why is this a good thing? First, it can capture the internal structure of words. The model can guess, for example, "dog" and "doggy" are somewhat related, even before seeing how they are used in context at all. Second, it is robust to unknown words that weren't encountered during the training.&lt;/p&gt;
&lt;h2&gt;How to use ELMo&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://allennlp.org/"&gt;AllenNLP&lt;/a&gt;, an open-source NLP platform developed by the Allen Institute of Artificial Intelligence, provides pre-trained ELMo models and interfaces that make it very easy for you to integrate ELMo with your model. In what follows, I'm going to demonstrate how to integrate ELMo embeddings with the sentiment analysis model I trained in the &lt;a href="training-sentiment-analyzer-using-allennlp.html"&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to integrate ELMo, you need to make only three changes to the script. Firstly, because ELMo uses a character-based CNN to encode words as mentioned above, you need to change how words are indexed when the Stanford Sentiment TreeBank is accessed. AllenNLP provides a convenient &lt;code&gt;ELMoTokenCharactersIndexer&lt;/code&gt; for this, which basically encodes a word by an array of its character IDs: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# In order to use ELMo, each word in a sentence needs to be indexed with&lt;/span&gt;
&lt;span class="c1"&gt;# an array of character IDs.&lt;/span&gt;
&lt;span class="n"&gt;elmo_token_indexer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ELMoTokenCharactersIndexer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StanfordSentimentTreeBankDatasetReader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;token_indexers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tokens&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;elmo_token_indexer&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Secondly, you need to create an &lt;code&gt;Embedder&lt;/code&gt; that actually embeds tokens using ELMo, and pass it to &lt;code&gt;BasicTextFieldEmbedder&lt;/code&gt;. All you need to do is instantiate an &lt;a href="https://allenai.github.io/allennlp-docs/api/allennlp.modules.token_embedders.html#elmo-token-embedder"&gt;&lt;code&gt;ElmoTokenEmbedder&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;Embedding&lt;/code&gt;. It takes two mandatory parameters upon instantiation — &lt;code&gt;options_file&lt;/code&gt; and &lt;code&gt;weight_file&lt;/code&gt; — which determine which of the four pre-trained ELMo models that AllenNLP provides you'd like to use to instantiate an &lt;code&gt;Embedder&lt;/code&gt;. The four pre-trained ELMo models basically differ in the size of the LSTM internal states and the output vectors. You can see the full specifications along with their URLs on &lt;a href="https://allennlp.org/elmo"&gt;their ELMo page&lt;/a&gt;. In this article, we are going to use the "Small" model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Use the &amp;#39;Small&amp;#39; pre-trained model&lt;/span&gt;
&lt;span class="n"&gt;options_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;https://s3-us-west-2.amazonaws.com/allennlp/models/elmo&amp;#39;&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;weight_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;https://s3-us-west-2.amazonaws.com/allennlp/models/elmo&amp;#39;&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;elmo_embedder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ElmoTokenEmbedder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;options_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Pass in the ElmoTokenEmbedder instance instead&lt;/span&gt;
&lt;span class="n"&gt;word_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BasicTextFieldEmbedder&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;elmo_embedder&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, you need to adjust the input vector size of your LSTM-RNN. By the way, this is not the biLM used by ELMo, but rather the LSTM you built to classify the sentence. Because we are using the ELMo embeddings as the input to this LSTM, you need to adjust the &lt;code&gt;input_size&lt;/code&gt; parameter to &lt;code&gt;torch.nn.LSTM&lt;/code&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# The dimension of the ELMo embedding will be 2 x [size of LSTM hidden states]&lt;/span&gt;
&lt;span class="n"&gt;elmo_embedding_dim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;
&lt;span class="n"&gt;lstm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PytorchSeq2VecWrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;elmo_embedding_dim&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_first&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The new &lt;code&gt;input_size&lt;/code&gt; will be 256 because the output vector size of the ELMo model we are using is 128, and there are two directions (forward and backward). &lt;/p&gt;
&lt;p&gt;And that's it! Here's &lt;a href="https://github.com/mhagiwara/realworldnlp/blob/master/examples/sentiment/sst_classifier_elmo.py"&gt;the entire script&lt;/a&gt; for training and testing an ELMo-augmented sentiment classifier on the Stanford Sentiment TreeBank dataset. If you run this script, you should get an accuracy of ~0.60 on the training set and ~0.40 on the dev set. If this doesn't sound like a huge improvement from the non-ELMo model we built last time, you are right — we only used the small model this time, and more importantly, we are &lt;em&gt;not&lt;/em&gt; using the linear combinations of all ELMo biLM layers, but rather just the topmost layer. If you read &lt;a href="https://arxiv.org/pdf/1802.05365.pdf"&gt;the original ELMo paper&lt;/a&gt;, you realize how important it is to use multiple layers. Each biLM layer represents different types of information and you need to optimize which layers to focus on depending on the task. To obtain all the layers from ELMo, you need to use &lt;a href="https://allenai.github.io/allennlp-docs/api/allennlp.modules.elmo.html#module-allennlp.modules.elmo"&gt;&lt;code&gt;ELMo&lt;/code&gt; class&lt;/a&gt; instead. &lt;/p&gt;
&lt;h2&gt;Configuring the Training Pipeline in JSON&lt;/h2&gt;
&lt;p&gt;Now, let's switch gears and study how we can do all this without writing a single line of Python code. One of the great features of AllenNLP is that it allows users to write JSON-based configuration files that completely specify how to train a model. Why is this great or even necessary? Didn't we just write an end-to-end specification of an experiment in Python?&lt;/p&gt;
&lt;p&gt;The first reason is that it encourages the separation between implementation and experiment metadata. If you have any experience training NLP models (or any ML models in general), you may have encountered a situations like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You start off creating a simple model with fixed values of hyperparameters.&lt;/li&gt;
&lt;li&gt;Your script is clean and simple.&lt;/li&gt;
&lt;li&gt;However, your model doesn't perform as well as you'd hoped, so you start tweaking those hyperparameters by changing the script directly.&lt;/li&gt;
&lt;li&gt;This improves the model performance to a certain degree.&lt;/li&gt;
&lt;li&gt;Still not satisfied, you start experimenting with different model architectures by replacing RNNs here and there with CNNs, using GRUs instead of LSTM, etc., again by making changes to the script directly.&lt;/li&gt;
&lt;li&gt;You may also tweak how the data is pre-processed by trying character-based embeddings instead of token-based ones, and by replacing the tokenizer with a different one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, your script is a huge mess of &lt;code&gt;if-then&lt;/code&gt;s and glue code that stitches together different components, and unless you are very careful, there's no way to keep track of what you've done so far in this journey. &lt;/p&gt;
&lt;p&gt;The second reason is the separation of dependency from implementation. In such messy code, chances are you have a huge, hairy &lt;code&gt;Model&lt;/code&gt; that has many sub-components in it. Working with such a as large &lt;code&gt;Model&lt;/code&gt; is painful and prone to errors, because it becomes progressively harder to make any changes to it while understanding their side effects completely. Also, sub-components of such huge models are usually tightly coupled, making it difficult to reuse the model itself outside the task in question. &lt;/p&gt;
&lt;p&gt;This separation of module dependency into an outside configuration file is a type of programming technique called &lt;a href="https://en.wikipedia.org/wiki/Dependency_injection"&gt;dependency injection&lt;/a&gt;, which improves the reusability of components and limits the side effect of code changes.  &lt;/p&gt;
&lt;p&gt;AllenNLP configuration files are written in &lt;a href="https://jsonnet.org/"&gt;Jsonnet&lt;/a&gt;, a superset of JSON with added functionalities such as variables and comments. For example, you can write variable declrations as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;local embedding_dim = 128;
local hidden_dim = 128;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First, you specify where the datasets come from and how to read them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;quot;dataset_reader&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;sst_tokens&amp;quot;,
  &amp;quot;token_indexers&amp;quot;: {
    &amp;quot;tokens&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;elmo_characters&amp;quot;
    }
  }
},
&amp;quot;train_data_path&amp;quot;: &amp;quot;data/stanfordSentimentTreebank/trees/train.txt&amp;quot;,
&amp;quot;validation_data_path&amp;quot;: &amp;quot;data/stanfordSentimentTreebank/trees/dev.txt&amp;quot;,
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;type&lt;/code&gt; key specifies the name of the instantiated class, and the rest of the keys correspond to the named parameters to the constructor. Then, you can specify your model as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;// In order to use a model in configuration, it must
//   1) inherit from the Registrable base class, and
//   2) be decorated by @Model.register(&amp;quot;model_name&amp;quot;).
// Also, the class has to be discoverable by the &amp;quot;allennlp&amp;quot; command
// by specifying &amp;#39;--include-package [import path]&amp;#39;.

&amp;quot;model&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;lstm_classifier&amp;quot;,

  // What&amp;#39;s going on here -
  // The `word_embeddings` parameter takes an instance of TextFieldEmbedder.
  // In the Python code, you instantiated a BasicTextFieldEmbedder and passed it to
  // `word_embeddings`. However, the default implementation of TextFieldEmbedder is
  // &amp;quot;basic&amp;quot;, which is BasicTextFieldEmbedder.
  // That&amp;#39;s why you can write parameters to BasicTextFieldEmbedder (dictionary from
  // field names to their embedder) directly here.

  &amp;quot;word_embeddings&amp;quot;: {
    &amp;quot;tokens&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;elmo_token_embedder&amp;quot;,
      &amp;quot;options_file&amp;quot;: &amp;quot;https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/[...].json&amp;quot;,
      &amp;quot;weight_file&amp;quot;: &amp;quot;https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/[...].hdf5&amp;quot;,
      &amp;quot;do_layer_norm&amp;quot;: false,
      &amp;quot;dropout&amp;quot;: 0.5
    }
  },

  // In Python code, you need to wrap encoders (e.g., torch.nn.LSTM) by PytorchSeq2VecWrapper.
  // Conveniently, &amp;quot;wrapped&amp;quot; version of popular encoder types (&amp;quot;lstm&amp;quot;, &amp;quot;gru&amp;quot;, ...)
  // are already registered (see https://github.com/allenai/allennlp/blob/master/allennlp/modules/seq2vec_encoders/__init__.py)
  // so you can just use them by specifying intuitive names

  &amp;quot;encoder&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;lstm&amp;quot;,
    &amp;quot;input_size&amp;quot;: embedding_dim,
    &amp;quot;hidden_size&amp;quot;: hidden_dim
  }
},
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, you can specify the iterator and the trainer used for the training:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;quot;iterator&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;bucket&amp;quot;,
  &amp;quot;batch_size&amp;quot;: 32,
  &amp;quot;sorting_keys&amp;quot;: [[&amp;quot;tokens&amp;quot;, &amp;quot;num_tokens&amp;quot;]]
},
&amp;quot;trainer&amp;quot;: {
  &amp;quot;optimizer&amp;quot;: &amp;quot;adam&amp;quot;,
  &amp;quot;num_epochs&amp;quot;: 20,
  &amp;quot;patience&amp;quot;: 10
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can see &lt;a href="https://github.com/mhagiwara/realworldnlp/blob/master/examples/sentiment/sst_classifier_elmo.json"&gt;the entire configuration file here&lt;/a&gt;, which can be run by the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;allennlp train examples/sentiment/sst_classifier_elmo.json \
    --serialization-dir sst-model \
    --include-package examples.sentiment.sst_classifier
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When you run this, you should see similar accuracy numbers as you saw for the Python script above.&lt;/p&gt;
&lt;p&gt;Once you finish training, you can create a test JSON file with one JSON-encoded instance per each line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;quot;tokens&amp;quot;: [&amp;quot;This&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;best&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;ever&amp;quot;, &amp;quot;!&amp;quot;]}
{&amp;quot;tokens&amp;quot;: [&amp;quot;This&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;worst&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;ever&amp;quot;, &amp;quot;!&amp;quot;]}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which you can feed into the prediction pipeline as below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;allennlp predict sst-model/model.tar.gz test.json \
    --include-package examples.sentiment.sst_classifier \
    --predictor sentence_classifier_predictor
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The predictor used here is the &lt;a href="https://github.com/mhagiwara/realworldnlp/blob/master/realworldnlp/predictors.py"&gt;one you defined and registered previously&lt;/a&gt;. Note that you need to register you predictor using the &lt;code&gt;@Predictor.register&lt;/code&gt; decorator instead of &lt;code&gt;@Model.register&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;I hope you enjoyed this little tutorial. This is a sample task from my book "Real-World Natural Language Processing", which is to be published in 2019 from Manning Publications. I'll post more information on &lt;a href="http://realworldnlpbook.com"&gt;the book website&lt;/a&gt; as I make progress on the book, so stay tuned!&lt;/p&gt;</content><category term="Sentiment Analysis"></category><category term="Word Embeddings"></category><category term="ELMo"></category><category term="AllenNLP"></category></entry><entry><title>Training a Sentiment Analyzer using AllenNLP (in less than 100 lines of Python code)</title><link href="http://www.realworldnlpbook.com/blog/training-sentiment-analyzer-using-allennlp.html" rel="alternate"></link><published>2018-10-13T00:00:00-04:00</published><updated>2018-10-13T00:00:00-04:00</updated><author><name>Masato Hagiwara</name></author><id>tag:www.realworldnlpbook.com,2018-10-13:/blog/training-sentiment-analyzer-using-allennlp.html</id><summary type="html">&lt;h2&gt;What is Sentiment Analysis?&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is a popular text analytic technique used in the automatic identification and categorization of subjective information within text. The technique is widely used in quantifying opinions, emotions, etc. that are usually written in an unstructured way; and thus, hard to quantify otherwise. Sentiment analysis …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;What is Sentiment Analysis?&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is a popular text analytic technique used in the automatic identification and categorization of subjective information within text. The technique is widely used in quantifying opinions, emotions, etc. that are usually written in an unstructured way; and thus, hard to quantify otherwise. Sentiment analysis is applied to a wide variety of textual resources such as survey, reviews, social media posts, and so on. &lt;/p&gt;
&lt;p&gt;One of the most basic tasks in sentiment analysis is the classification of polarity, that is, to classify whether the expressed opinion is positive, negative, or neutral. There could be more than three classes, e.g., strongly positive, positive, neutral, negative, or strongly negative. This may sound familiar to you if used one of the websites (think: Amazon) where people can review things (products, movies, anything) using a 5-point scale expressed by the number of stars. &lt;/p&gt;
&lt;h2&gt;Stanford Sentiment TreeBank&lt;/h2&gt;
&lt;p&gt;There are several publicly available datasets for sentiment classification. In this post, we're going to use the &lt;a href="https://nlp.stanford.edu/sentiment/"&gt;Stanford Sentiment TreeBank&lt;/a&gt;, or abbreviated as SST, which is probably one of the most widely-used sentiment datasets as of today. One feature that differentiates SST from other datasets is the fact that sentiment labels are assigned not only to sentences but also to every phrase, and every word, in sentences. This enables us to study the complex semantic interactions between words and phrases. For example, let's consider the polarity of this sentence as a whole:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This movie was actually neither that funny, nor super witty.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above statement would definitely be a negative, although, if you focus on the individual words (such as &lt;em&gt;funny&lt;/em&gt;, &lt;em&gt;witty&lt;/em&gt;), you might be fooled to think it's a positive. A naive bag-of-words classifier which focuses solely on individual words would have difficulties classifying this example correctly. In order to correctly classify the polarity of this sentence, you need to understand the semantic impact of the negation "neither ... nor ...". For this property, SST has been used as the standard benchmark for neural network models that can capture the syntactic structures of sentence [&lt;a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"&gt;Socher et al. 2013&lt;/a&gt;].   &lt;/p&gt;
&lt;h2&gt;PyTorch and AllenNLP&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; is my favorite deep learning framework. It provides flexible, easy-to-write modules that you can run dynamically while being reasonably fast. The use of PyTorch within the research community &lt;a href="https://www.reddit.com/r/MachineLearning/comments/9kys38/r_frameworks_mentioned_iclr_20182019_tensorflow/"&gt;has exploded in the past year&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although PyTorch is a very powerful framework, natural language processing often involves low-level, boilerplate chores, including, but not limited to: reading and writing datasets, tokenizing and indexing words, managing vocabulary, minibatching, sorting and padding, etc. Although correctly having such building blocks is crucial in NLP tasks, you will need to write similar design patterns again and again when you're iterating fast, which could be time-wasting. This is where libraries like AllenNLP proves reliable.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://allennlp.org/"&gt;AllenNLP&lt;/a&gt; is an open-source NLP platform developed by the Allen Institute of Artificial Intelligence. It was designed to support quick iterations for NLP research and development, especially for semantic and language understanding tasks. It provides a flexible API, useful abstractions for NLP, and a modular experimental framework that accelerates NLP research. &lt;/p&gt;
&lt;p&gt;In this post, I'm going to show you a step-by-step guide of how to build your own sentiment classifier using AllenNLP. Because AllenNLP takes care of the low-level chores and provides the training framework, the entire script is &lt;a href="https://github.com/mhagiwara/realworldnlp/blob/master/examples/sentiment/sst_classifier.py"&gt;less than just 100 lines of Python code&lt;/a&gt;. If necessary, you could experiment with other network architectures quite easily. &lt;/p&gt;
&lt;p&gt;Go ahead and download the SST dataset. What you'll need is the dataset split into train, dev, and testsets in PTB tree format which can be downloaded from &lt;a href="https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip"&gt;this link&lt;/a&gt;. We assume that those files are expanded under &lt;code&gt;data/stanfordSentimentTreebank/trees&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;Note that in the code snippets below, we assume that you already &lt;code&gt;import&lt;/code&gt;ed appropriate modules, classes, and methods. See the &lt;a href="https://github.com/mhagiwara/realworldnlp/blob/master/examples/sentiment/sst_classifier.py"&gt;full script&lt;/a&gt; for more details. By the way, you'll notice that this script is very similar to the AllenNLP's &lt;a href="https://allennlp.org/tutorials"&gt;part-of-speech tagging tutorial&lt;/a&gt;. It is very easy to experiment with different models and tasks with little modification in AllenNLP.  &lt;/p&gt;
&lt;h2&gt;Reading and Pre-Processing Dataset&lt;/h2&gt;
&lt;p&gt;AllenNLP already provides a handy dataset reader called &lt;code&gt;StanfordSentimentTreeBankDatasetReader&lt;/code&gt; --- an interface for reading the SST dataset. You can read the dataset by specifying the path to the dataset files as the argument for the &lt;code&gt;read()&lt;/code&gt; method as in: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StanfordSentimentTreeBankDatasetReader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data/stanfordSentimentTreebank/trees/train.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dev_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data/stanfordSentimentTreebank/trees/dev.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first step in almost any deep NLP tasks is to specify how to convert textual data into tensors. This comprises a step in which words and labels (in this case, polarity labels such as positive and negative) are converted to integer IDs. In AllenNLP, this is automatically taken care of by &lt;code&gt;Vocabulary&lt;/code&gt;, which stores the mapping from words/labels to IDs.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# You can optionally specify the minimum count of tokens/labels.&lt;/span&gt;
&lt;span class="c1"&gt;# `min_count={&amp;#39;tokens&amp;#39;:3}` here means that any tokens that appear less than three times&lt;/span&gt;
&lt;span class="c1"&gt;# will be ignored and not included in the vocabulary.&lt;/span&gt;
&lt;span class="n"&gt;vocab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Vocabulary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_instances&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;dev_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                  &lt;span class="n"&gt;min_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tokens&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The next step in many deep NLP tasks is to convert words into an embedding. In deep learning, an embedding is a continuous vector representation of something that is usually discrete and high dimensional. You can use &lt;code&gt;Embedding&lt;/code&gt; to create this mapping and use &lt;code&gt;BasicTextFieldEmbedder&lt;/code&gt; to actually convert IDs into embedded vectors. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;token_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_embeddings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_vocab_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tokens&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                            &lt;span class="n"&gt;embedding_dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,&lt;/span&gt;
&lt;span class="c1"&gt;# not for labels, which are used unchanged as the answer of the sentence classification&lt;/span&gt;
&lt;span class="n"&gt;word_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BasicTextFieldEmbedder&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;token_embedding&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Sentence Classification Model&lt;/h2&gt;
&lt;figure style="text-align: center"&gt;
    &lt;img src="images/lstm_sst.png"/&gt;
    &lt;figcaption&gt;Figure: LSTM-RNN Sentence Classification Model&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Now, define our model that classifies a sentence into classes. The model is a standard LSTM-RNN plus a fully connected linear layer for classification. If this seems like a lot, don't worry, I've added extensive comments in the snippet:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Model in AllenNLP represents a model that is trained.&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LstmClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;word_embeddings&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TextFieldEmbedder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Seq2VecEncoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Vocabulary&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# We need the embeddings to convert word IDs to their vector representations&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_embeddings&lt;/span&gt;

        &lt;span class="c1"&gt;# Seq2VecEncoder is a neural network abstraction that takes a sequence of something&lt;/span&gt;
        &lt;span class="c1"&gt;# (usually a sequence of embedded word vectors), processes it, and returns it as a single&lt;/span&gt;
        &lt;span class="c1"&gt;# vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but&lt;/span&gt;
        &lt;span class="c1"&gt;# AllenNLP also supports CNNs and other simple architectures (for example,&lt;/span&gt;
        &lt;span class="c1"&gt;# just averaging over the input vectors).&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;

        &lt;span class="c1"&gt;# After converting a sequence of vectors to a single vector, we feed it into&lt;/span&gt;
        &lt;span class="c1"&gt;# a fully-connected linear layer to reduce the dimension to the total number of labels.&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden2tag&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_output_dim&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
                                          &lt;span class="n"&gt;out_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_vocab_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CategoricalAccuracy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="c1"&gt;# We use the cross-entropy loss because this is a classification task.&lt;/span&gt;
        &lt;span class="c1"&gt;# Note that PyTorch&amp;#39;s CrossEntropyLoss combines softmax and log likelihood loss,&lt;/span&gt;
        &lt;span class="c1"&gt;# which makes it unnecessary to add a separate softmax layer.&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# Instances are fed to forward after batching.&lt;/span&gt;
    &lt;span class="c1"&gt;# Fields are passed through arguments with the same name.&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# In deep NLP, when sequences of tensors in different lengths are batched together,&lt;/span&gt;
        &lt;span class="c1"&gt;# shorter sequences get padded with zeros to make them of equal length.&lt;/span&gt;
        &lt;span class="c1"&gt;# Masking is the process to ignore extra zeros added by padding&lt;/span&gt;
        &lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_text_field_mask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Forward pass&lt;/span&gt;
        &lt;span class="n"&gt;embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;word_embeddings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;encoder_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden2tag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder_out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# In AllenNLP, the output of forward() is a dictionary.&lt;/span&gt;
        &lt;span class="c1"&gt;# Your output dictionary must contain a &amp;quot;loss&amp;quot; key for your model to be trained.&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;logits&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loss&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The key here is to &lt;code&gt;Seq2VecEncoder&lt;/code&gt;, which basically takes a sequence of tensors, and returns a single vector. We use an LSTM-RNN implementation as the encoder (Take a look at the documentation for &lt;a href="https://allenai.github.io/allennlp-docs/api/allennlp.modules.seq2vec_encoders.html#allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper"&gt;PytorchSeq2VecWrapper&lt;/a&gt; for why we need it):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;lstm = PytorchSeq2VecWrapper(
    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))

model = LstmClassifier(word_embeddings, lstm, vocab)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;Once you define the model, the rest of the training process is fairly easy. This is where high-level frameworks such as AllenNLP shine. Instead of writing tedious batching and training loops (as you'd do with PyTorch and TensorFlow), you just specify how to iterate through data and pass necessary arguments to the trainer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight_decay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BucketIterator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sorting_keys&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tokens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;num_tokens&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index_with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;validation_dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dev_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;patience&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;num_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;BucketIterator&lt;/code&gt; here sorts the training instances by the number of tokens so that instances in similar lengths end up in the same batch. Note that we also pass the validation dataset for early stopping.&lt;/p&gt;
&lt;p&gt;If you run this for 20 epochs, you should get an accuracy of ~ 0.78 on the training set and ~ 0.35 on the dev set. This may sound very low, but note that this is a 5-class classification problem and the random baseline accuracy is only 0.20.&lt;/p&gt;
&lt;h2&gt;Testing&lt;/h2&gt;
&lt;p&gt;In order to test whether or not the model you just trained is working as expected, you will need a predictor. A &lt;code&gt;Predictor&lt;/code&gt; is a class that provides JSON-based interfaces for passing the data to/from your model. I went ahead and wrote &lt;a href="https://github.com/mhagiwara/realworldnlp/blob/master/realworldnlp/predictors.py#L10"&gt;&lt;code&gt;SentenceClassifierPredictor&lt;/code&gt;&lt;/a&gt; which acts as a JSON-based interface to the sentence classification model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;This&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;is&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;movie&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;ever&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;!&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;predictor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SentenceClassifierPredictor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset_reader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;reader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;predictor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;logits&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;label_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_token_from_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You should see &lt;code&gt;4&lt;/code&gt;as a result of running this snippet. 4 is the label corresponding to "very positive". So, the model you just trained correctly predicted that this is a very positive movie review!&lt;/p&gt;
&lt;p&gt;And that's it for now. I hope you enjoyed this little tutorial. From next time, I'll explore the use of ELMo and also JSON-based configuration files in AllenNLP. This is a sample tutorial from my book "Real-World Natural Language Processing", which is to be published in 2019 from Manning Publications. I'll post more information on &lt;a href="http://realworldnlpbook.com"&gt;the book website&lt;/a&gt; as I make progress on the book, so stay tuned!&lt;/p&gt;</content><category term="Sentiment Analysis"></category><category term="AllenNLP"></category></entry></feed>